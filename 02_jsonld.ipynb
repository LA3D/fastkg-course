{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Working with JSON-LD\"\n",
    "author: \"Charles F. Vardeman II\"\n",
    "date: \"2023-09-12\"\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "        grid:\n",
    "          margin-width: 350px\n",
    "reference-location: margin\n",
    "citation-location: margin\n",
    "bibliography: kg.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uning JSON-LD and Generative AI to construct a Knowledge Graph\n",
    "We have been using the [kglab tutorial](https://derwen.ai/docs/kgl/tutorial/) to learn about using W3C web standards to construct knowledge graphs. One particularly useful standard that is briefly touched on in the tutorial is JSON-LD in the [serialization exercise](https://derwen.ai/docs/kgl/ex1_0/#serialization-as-json-ld). This corresponds to [building a knowledge graph with rdflib](kglab/examples/ex1_0.ipynb) in the kglab/examples subdirectory.\n",
    "\n",
    "## Using Generative AI to create column types.\n",
    "[@koriniColumnTypeAnnotation2023]\n",
    "\n",
    "----\n",
    "**ChatGPT Advanced Data Analysis:** \n",
    "\n",
    "Absolutely, having a summary and a set of steps can make the process easily reproducible. Here it is:\n",
    "\n",
    "## Summary\n",
    "\n",
    "To generate Turtle data from a Pandas DataFrame:\n",
    "\n",
    "1. **Pandas to JSON**: Convert the DataFrame to JSON format using `to_json()` with `orient='records'`.\n",
    "\n",
    "2. **Add UUID and Type to JSON**: Iterate through the JSON objects and add a UUID prefixed by `urn:uuid:` and a `@type` with the value `Person`.\n",
    "\n",
    "3. **Create JSON-LD Context**: Define a JSON-LD context to map JSON keys to RDF vocabularies. Include mappings for the UUID and `rdf:type`.\n",
    "\n",
    "4. **Combine to Form JSON-LD**: Create a JSON-LD object that incorporates both the JSON data and the JSON-LD context. Put the JSON objects inside a `@graph` array.\n",
    "\n",
    "5. **Load into RDFlib**: Use RDFlib's `parse()` method to load the JSON-LD data into an RDF graph.\n",
    "\n",
    "6. **Serialize to Turtle**: Use RDFlib's `serialize()` method to convert the RDF graph to Turtle format.\n",
    "\n",
    "## Code Prompt\n",
    "\n",
    "Here's a condensed Python code snippet that includes all the steps:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from rdflib import Graph\n",
    "\n",
    "# Step 1: Convert DataFrame to JSON\n",
    "data = {'Name': ['Alice', 'Bob'], 'Age': [25, 30], 'Occupation': ['Engineer', 'Doctor']}\n",
    "df = pd.DataFrame(data)\n",
    "json_data = df.to_json(orient='records')\n",
    "\n",
    "# Step 2: Add UUID and Type to JSON\n",
    "json_data_dict = json.loads(json_data)\n",
    "for obj in json_data_dict:\n",
    "    obj[\"UUID\"] = \"urn:uuid:\" + str(uuid.uuid4())\n",
    "    obj[\"@type\"] = \"Person\"\n",
    "\n",
    "# Step 3: Create JSON-LD Context\n",
    "context = {\n",
    "    \"@context\": {\n",
    "        \"UUID\": \"@id\",\n",
    "        \"Name\": \"http://schema.org/name\",\n",
    "        \"Age\": \"http://schema.org/age\",\n",
    "        \"Occupation\": \"http://schema.org/occupation\",\n",
    "        \"@type\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\n",
    "        \"Person\": \"http://xmlns.com/foaf/0.1/Person\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 4: Combine to Form JSON-LD\n",
    "json_ld_data = {\n",
    "    \"@context\": context[\"@context\"],\n",
    "    \"@graph\": json_data_dict\n",
    "}\n",
    "\n",
    "# Step 5: Load into RDFlib\n",
    "g = Graph()\n",
    "g.parse(data=json.dumps(json_ld_data), format='json-ld')\n",
    "\n",
    "# Step 6: Serialize to Turtle\n",
    "turtle_data = g.serialize(format='turtle')\n",
    "print(turtle_data)\n",
    "```\n",
    "\n",
    "This will generate Turtle data that includes UUIDs and a `rdf:type` of `Person`, aligning with your requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### Transforming Standard JSON to a Knowledge Graph\n",
    "The beauty of JSON-LD lies in its ability to transform standard JSON data into a knowledge graph simply and efficiently. By adding a '@context' to a JSON document, developers can define how the data should be interpreted semantically. This turns a flat data structure into an interconnected web of information, opening up powerful querying and linking possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our general procedure towards automated KG Construction using LLMs\n",
    "I understand the workflow you've outlined for constructing a knowledge graph from CSV data using JSON-LD. The process includes six major steps:\n",
    "\n",
    "1. **ETL from CSV to JSON**: Extract, transform, and load the data from a CSV file into a JSON object. Here, we'll use Python libraries like pandas.\n",
    "  \n",
    "2. **Creating JSON-LD Vocabulary File**: Establish the terms, data types, and definitions in a JSON-LD vocabulary file based on the LLM template.\n",
    "  \n",
    "3. **Convert from JSON to JSON-LD**: Extend the JSON data to JSON-LD by adding the context, linking it with the vocabulary's IRI, wrapping the data in a @graph object, and assigning unique IDs.\n",
    "  \n",
    "4. **Introduce New Ontology File**: Create another JSON-LD vocabulary file to serve as an ontology.\n",
    "  \n",
    "5. **Link Vocabularies in Another JSON-LD File**: Establish relationships between the vocabularies, definitions, and ontologies in a separate JSON-LD file.\n",
    "  \n",
    "6. **Capturing Provenance Information**: Document metadata about the @graph object to capture its origin and transformation history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Using JSON-LD to construct a Knowledge Graph from Wikipedia tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia can provide a starting point for structuring and enriching knowledge graphs. For our Navy project, we will want to provide LLM Agents with context for factual information that may have changed since \"pre-training\" using [`Retrieval Augmented Generation`](https://learn.microsoft.com/en-us/azure/machine-learning/concept-retrieval-augmented-generation?view=azureml-api-2#Technical%20overview%20of%20using%20RAG%20on%20Large%20Language%20Models%20(LLMs)). For our purposes, we want to augment our KG with broader contextual information that may be useful to agents for question and answering. One set of context, is the [List of current ships of the United States Navy](https://en.wikipedia.org/wiki/List_of_current_ships_of_the_United_States_Navy) that contain a number of tables that could be useful to add to our knowledge graph. [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) does contain most of the `ships` but the entities are not always complete. For example, the Wikidata entity page for [USS Abraham Lincoln](https://www.wikidata.org/wiki/Q587557) shows a rather rich set of relations including `events` that correspond to `ship naming ceremony`, `ship commissioning`. However, ships like [USS Carter Hall](https://www.wikidata.org/wiki/Q2468798) are very basic and don't contain a lot of information we can extract. Looking at the [History for `List of current ships of the United States Navy`](https://en.wikipedia.org/w/index.php?title=List_of_current_ships_of_the_United_States_Navy&action=history) tells us that the page is fairly active and Wikipedia contributors are keeping it more up to date than the Wikidata entries.\n",
    "\n",
    "Lets' do a little [Exploratory Data Analysis](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners) using large language models similar to [Getting Started With LLMs](https://www.kaggle.com/code/jhoward/getting-started-with-llms/). We will use [Beautiful Soup](https://pypi.org/project/beautifulsoup4/) to retreive the Wikipedia page and find the \"html tables\" in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_current_ships_of_the_United_States_Navy\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find_all(\"table\")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tables, lets create a pandas dataframe from the table and sanity check that the table is the same as [what is in the first table](https://en.wikipedia.org/wiki/List_of_current_ships_of_the_United_States_Navy#Commissioned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43085/3411228357.py:7: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ship name</th>\n",
       "      <th>Hull number</th>\n",
       "      <th>Class</th>\n",
       "      <th>Type</th>\n",
       "      <th>Commission date</th>\n",
       "      <th>Homeport[2]</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USS Abraham Lincoln</td>\n",
       "      <td>CVN-72</td>\n",
       "      <td>Nimitz</td>\n",
       "      <td>Aircraft carrier</td>\n",
       "      <td>11 November 1989</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USS Alabama</td>\n",
       "      <td>SSBN-731</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Ballistic missile submarine</td>\n",
       "      <td>25 May 1985</td>\n",
       "      <td>Bangor, WA</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USS Alaska</td>\n",
       "      <td>SSBN-732</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Ballistic missile submarine</td>\n",
       "      <td>25 January 1986</td>\n",
       "      <td>Kings Bay, GA</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USS Albany</td>\n",
       "      <td>SSN-753</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Attack submarine</td>\n",
       "      <td>7 April 1990</td>\n",
       "      <td>Norfolk, VA</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USS Alexandria</td>\n",
       "      <td>SSN-757</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Attack submarine</td>\n",
       "      <td>29 June 1991</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>[7] Scheduled to be decommissioned 2026[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>USS William P. Lawrence</td>\n",
       "      <td>DDG-110</td>\n",
       "      <td>Arleigh Burke</td>\n",
       "      <td>Destroyer</td>\n",
       "      <td>19 May 2011</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>[241]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>USS Winston S. Churchill</td>\n",
       "      <td>DDG-81</td>\n",
       "      <td>Arleigh Burke</td>\n",
       "      <td>Destroyer</td>\n",
       "      <td>10 March 2001</td>\n",
       "      <td>Norfolk, VA</td>\n",
       "      <td>[242]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>USS Wichita</td>\n",
       "      <td>LCS-13</td>\n",
       "      <td>Freedom</td>\n",
       "      <td>Littoral combat ship</td>\n",
       "      <td>12 January 2019</td>\n",
       "      <td>Mayport, FL</td>\n",
       "      <td>[243] Proposed to be decommissioned 2023[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>USS Wyoming</td>\n",
       "      <td>SSBN-742</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Ballistic missile submarine</td>\n",
       "      <td>13 July 1996</td>\n",
       "      <td>Kings Bay, GA</td>\n",
       "      <td>[244]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>USS Zumwalt</td>\n",
       "      <td>DDG-1000</td>\n",
       "      <td>Zumwalt</td>\n",
       "      <td>Destroyer</td>\n",
       "      <td>15 October 2016</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>[245]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Ship name Hull number          Class  \\\n",
       "0         USS Abraham Lincoln      CVN-72         Nimitz   \n",
       "1                 USS Alabama    SSBN-731           Ohio   \n",
       "2                  USS Alaska    SSBN-732           Ohio   \n",
       "3                  USS Albany     SSN-753    Los Angeles   \n",
       "4              USS Alexandria     SSN-757    Los Angeles   \n",
       "..                        ...         ...            ...   \n",
       "233   USS William P. Lawrence     DDG-110  Arleigh Burke   \n",
       "234  USS Winston S. Churchill      DDG-81  Arleigh Burke   \n",
       "235               USS Wichita      LCS-13        Freedom   \n",
       "236               USS Wyoming    SSBN-742           Ohio   \n",
       "237               USS Zumwalt    DDG-1000        Zumwalt   \n",
       "\n",
       "                            Type   Commission date    Homeport[2]  \\\n",
       "0               Aircraft carrier  11 November 1989  San Diego, CA   \n",
       "1    Ballistic missile submarine       25 May 1985     Bangor, WA   \n",
       "2    Ballistic missile submarine   25 January 1986  Kings Bay, GA   \n",
       "3               Attack submarine      7 April 1990    Norfolk, VA   \n",
       "4               Attack submarine      29 June 1991  San Diego, CA   \n",
       "..                           ...               ...            ...   \n",
       "233                    Destroyer       19 May 2011  San Diego, CA   \n",
       "234                    Destroyer     10 March 2001    Norfolk, VA   \n",
       "235         Littoral combat ship   12 January 2019    Mayport, FL   \n",
       "236  Ballistic missile submarine      13 July 1996  Kings Bay, GA   \n",
       "237                    Destroyer   15 October 2016  San Diego, CA   \n",
       "\n",
       "                                             Note  \n",
       "0                                             [3]  \n",
       "1                                             [4]  \n",
       "2                                             [5]  \n",
       "3                                             [6]  \n",
       "4      [7] Scheduled to be decommissioned 2026[8]  \n",
       "..                                            ...  \n",
       "233                                         [241]  \n",
       "234                                         [242]  \n",
       "235  [243] Proposed to be decommissioned 2023[17]  \n",
       "236                                         [244]  \n",
       "237                                         [245]  \n",
       "\n",
       "[238 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| label: fig-commissioned\n",
    "#| fig-cap: Dataframe from Wikipedia Commissioned ship table.\n",
    "#| echo: true\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_html(str(table))[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we want to use the column names as the basis for eventually constructing a URI, we unfortunately need to make it web safe and remove spaces and other issues. The other question is \"what does a row\" represent? If we want to build a knowledge graph of all of the ships of the navy, we might want to consider a basic ontology to start with. The first table is \"commissioned\" ships and the idea of `commisioned` is a role relationship since it has a start time and a end time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship name',\n",
       " 'Hull number',\n",
       " 'Class',\n",
       " 'Type',\n",
       " 'Commission date',\n",
       " 'Homeport[2]',\n",
       " 'Note']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = df.columns.tolist()\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we construct the KG, we need to do a bit of data cleaning so we have a properly constructed Knowledge Graph. We will use ChatGPT \"Advanced Data Analysis\" as a surrogate for a Large Language Model based Agent.\n",
    "\n",
    "---\n",
    "\n",
    "**Prompt:** Before we proceed, lets think critically about the dataframe structure. We will use the to_json method to construct a JSON document and then combine it with the context. However, there are some issues with the data, and in particular the format of dates in the dataframe. Think critically and look for issues in the pandas dataframe that will result in inconsistent or invalid KG construction.\n",
    "\n",
    "---\n",
    "\n",
    "**ChatGPT:** You're correct; a critical review of the DataFrame structure is essential before proceeding to the knowledge graph construction. This will help ensure that the resulting KG is both consistent and valid. Here are some aspects to consider:\n",
    "\n",
    "### Issues with the DataFrame:\n",
    "\n",
    "1. **Date Format**: The \"Commission date\" column is formatted as a string in \"YYYY-MM-DD\" format. While the `to_json` method would convert it into a string in JSON, it's crucial that the context file specifies the date's type as \"xsd:date\" to ensure that it's interpreted correctly in the resulting KG.\n",
    "\n",
    "2. **Notes Column**: The \"Note\" column contains bracketed numbers, which could be references or footnotes. The meaning is unclear, and depending on the intended use, this could introduce ambiguity into the KG.\n",
    "\n",
    "3. **Homeport Formatting**: The \"Homeport[2]\" column contains location data as a string, but it's not in a standardized geographical format. This could be an issue if geographical querying is required later.\n",
    "\n",
    "4. **Column Names**: The column name \"Homeport[2]\" includes a special character, which may require special handling in both JSON and RDF. It's generally good practice to use simple, descriptive column names.\n",
    "\n",
    "5. **Uniqueness**: The \"Hull number\" is assumed to be a unique identifier for each ship, but the DataFrame doesn't enforce this. If there are duplicates, this would result in invalid KG construction.\n",
    "\n",
    "6. **Implicit Relationships**: The data implicitly suggests relationships (e.g., a ship belongs to a class and type), but these are not explicitly modeled in the DataFrame. They will be modeled in the KG, but it's good to be aware of these.\n",
    "\n",
    "7. **Role Modeling**: Since we decided to model \"Commissioned Ship\" as a role, the DataFrame structure doesn't capture this. Specifically, roles like \"Commissioned Ship\" often have start and end dates, but our DataFrame only has the start date (\"Commission date\").\n",
    "\n",
    "8. **Missing Data**: The DataFrame doesn't indicate missing or null values explicitly, which could be an issue when constructing the KG.\n",
    "\n",
    "9. **Data Types**: While pandas will infer the data types, it's good to explicitly set them, especially for columns that should be interpreted as categorical data (e.g., \"Class\", \"Type\").\n",
    "\n",
    "Addressing these issues before KG construction will help ensure that the resulting KG is both valid and useful for downstream applications. Would you like to address these issues step-by-step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Class': 'Nimitz',\n",
      " 'Commission_date': '11 November 1989',\n",
      " 'Homeport2': 'San Diego, CA',\n",
      " 'Hull_number': 'CVN-72',\n",
      " 'Note': '[3]',\n",
      " 'Ship_name': 'USS\\xa0Abraham Lincoln',\n",
      " 'Type': 'Aircraft carrier'}\n"
     ]
    }
   ],
   "source": [
    "#| label: fig-pandasJSON\n",
    "#| fig-cap: JSON generated directly from Pandas dataframe.\n",
    "#| echo: true\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Normalize column names in DataFrame\n",
    "normalized_columns = {col: col.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\") for col in column_names}\n",
    "df.rename(columns=normalized_columns, inplace=True)\n",
    "\n",
    "# Let's change the date to be consistent with xsd:date\n",
    "# df['Commission_date'] = pd.to_datetime(df['Commission_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert DataFrame to JSON-formatted string\n",
    "df_json_str = df.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "# Convert JSON-formatted string to Python object\n",
    "df_json = json.loads(df_json_str)\n",
    "\n",
    "# Pretty-print the first record\n",
    "pprint(df_json[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this JSON version of the first row of the dataframe, we can see some small data problems that we might want to address. First the \"Commission_data\" is not in an xsd:date format our knowledge graph would expect for a date literal. Second, the \"note\" field isn't very useful without the note. For the \"Null_number\", Homeport2, Class, Type and Ship Name, we will eventually want to align to wikidata properties. Remember, the Column names serve as properties to link \"whatever\" the row semantics are to the literal values of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Class': 'Nimitz',\n",
      " 'Commission_date': '1989-11-11',\n",
      " 'Homeport': 'San Diego, CA',\n",
      " 'Hull_number': 'CVN-72',\n",
      " 'Ship_name': 'USS Abraham Lincoln',\n",
      " 'Type': 'Aircraft carrier'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Data Cleaning Steps\n",
    "# 1. Replace special characters in 'Ship_name'\n",
    "df['Ship_name'] = df['Ship_name'].str.replace('\\xa0', ' ')\n",
    "\n",
    "# 2. Standardize 'Commission_date' to ISO 8601 format\n",
    "df['Commission_date'] = pd.to_datetime(df['Commission_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3. Remove 'Note' field\n",
    "df = df.drop(columns=['Note'])\n",
    "\n",
    "# 4. Standardize 'Homeport' field name\n",
    "df.rename(columns={'Homeport2': 'Homeport'}, inplace=True)\n",
    "\n",
    "# Convert cleaned DataFrame to JSON\n",
    "json_data_cleaned = df.to_json(orient='records')\n",
    "\n",
    "# Convert JSON-formatted string to Python object\n",
    "df_json = json.loads(json_data_cleaned)\n",
    "\n",
    "# Pretty-print the first record\n",
    "pprint(df_json[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Conventions for naming in RDF\n",
    "\n",
    "The prevalent practice in RDF ontology naming is to employ [Camel case](https://en.wikipedia.org/wiki/Camel_case). Specifically, **lower camel case** is preferred for relationships, which include properties and edges in the knowledge graph. In contrast, **upper camel case** is advised for Classes and types, representing nodes in the knowledge graph. It's worth noting that while camel case serves as an implicit identifier, more explicit identifiers such as `rdf:Property` are also commonly used, especially in the context of large language models. \n",
    "\n",
    "**For example:**\n",
    "```json\n",
    "{\n",
    "  \"@context\": {\n",
    "    \"@vocab\": \"http://schema.org/\",\n",
    "    \"name\": {\n",
    "      \"@id\": \"rdf:Property\",\n",
    "      \"@type\": \"xsd:string\"\n",
    "    }\n",
    "  },\n",
    "  \"@type\": \"Person\",\n",
    "  \"name\": \"John Doe\"\n",
    "}\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "To generate a RDF knowledge graph from the JSON, we also need a @context that defines our JSON `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-turtle\n",
    "#| fig-cap: JSON generated directly from Pandas dataframe.\n",
    "#| echo: true\n",
    "\n",
    "from rdflib import Graph, URIRef\n",
    "\n",
    "# Step 1: Add UUID and Type to JSON\n",
    "json_data_cleaned_dict = json.loads(json_data_cleaned)\n",
    "for obj in json_data_cleaned_dict:\n",
    "    obj[\"UUID\"] = \"urn:uuid:\" + str(uuid.uuid4())\n",
    "    obj[\"@type\"] = \"Ship\"\n",
    "\n",
    "# Step 2: Create JSON-LD Context with SKOS definitions and XSD literals\n",
    "context_with_skos_and_xsd = {\n",
    "    \"@context\": {\n",
    "        \"UUID\": \"@id\",\n",
    "        \"Class\": {\n",
    "            \"@id\": \"http://example.org/Class\",\n",
    "            \"@type\": \"xsd:string\",\n",
    "            \"skos:definition\": \"The class of the ship.\"\n",
    "        },\n",
    "        \"Commission_date\": {\n",
    "            \"@id\": \"http://example.org/Commission_date\",\n",
    "            \"@type\": \"xsd:date\",\n",
    "            \"skos:definition\": \"The date the ship was commissioned.\"\n",
    "        },\n",
    "        \"Homeport\": {\n",
    "            \"@id\": \"http://example.org/Homeport\",\n",
    "            \"@type\": \"xsd:string\",\n",
    "            \"skos:definition\": \"The homeport of the ship.\"\n",
    "        },\n",
    "        \"Hull_number\": {\n",
    "            \"@id\": \"http://example.org/Hull_number\",\n",
    "            \"@type\": \"xsd:string\",\n",
    "            \"skos:definition\": \"The hull number of the ship.\"\n",
    "        },\n",
    "        \"Ship_name\": {\n",
    "            \"@id\": \"http://example.org/Ship_name\",\n",
    "            \"@type\": \"xsd:string\",\n",
    "            \"skos:definition\": \"The name of the ship.\"\n",
    "        },\n",
    "        \"Type\": {\n",
    "            \"@id\": \"http://example.org/Type\",\n",
    "            \"@type\": \"xsd:string\",\n",
    "            \"skos:definition\": \"The type of the ship.\"\n",
    "        },\n",
    "        \"@type\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\n",
    "        \"Ship\": \"http://example.org/Ship\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 3: Combine to Form JSON-LD\n",
    "json_ld_data_cleaned = {\n",
    "    \"@context\": context_with_skos_and_xsd[\"@context\"],\n",
    "    \"@graph\": json_data_cleaned_dict\n",
    "}\n",
    "\n",
    "# Step 4: Load into RDFlib\n",
    "g_cleaned = Graph()\n",
    "g_cleaned.parse(data=json.dumps(json_ld_data_cleaned), format='json-ld')\n",
    "\n",
    "# Step 5: Serialize to Turtle\n",
    "turtle_data = g_cleaned.serialize(format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ns1: <http://example.org/> .\n",
      "<urn:uuid:0080c026-d273-450a-a687-5482a792447a> a ns1:Ship ;\n",
      "    ns1:Class \"Ticonderoga\"^^<xsd:string> ;\n",
      "    ns1:Commission_date \"1991-01-12\"^^<xsd:date> ;\n",
      "    ns1:Homeport \"Pearl Harbor, HI\"^^<xsd:string> ;\n",
      "    ns1:Hull_number \"CG-65\"^^<xsd:string> ;\n",
      "    ns1:Ship_name \"USS Chosin\"^^<xsd:string> ;\n",
      "    ns1:Type \"Cruiser\"^^<xsd:string> .\n",
      "<urn:uuid:00d02856-4403-45c4-a056-1e9e7ec826ee> a ns1:Ship ;\n",
      "    ns1:Class \"Arleigh Burke\"^^<xsd:string> ;\n",
      "    ns1:Commission_date \"1995-10-14\"^^<xsd:date> ;\n",
      "    ns1:Homeport \"San Diego, CA\"^^<xsd:string> ;\n",
      "    ns1:Hull_number \"DDG-62\"^^<xsd:string> ;\n",
      "    ns1:Ship_name \"USS Fitzgerald\"^^<xsd:string> ;\n",
      "    ns1:Type \"Destroyer\"^^<xsd:string> .\n"
     ]
    }
   ],
   "source": [
    "#| label: fig-turtleout\n",
    "#| fig-cap: Knowledge Graph output in Turtle.\n",
    "#| echo: false\n",
    "\n",
    "# Initialize variables\n",
    "result = []\n",
    "current_statement = []\n",
    "count_statements = 0\n",
    "\n",
    "# Parse lines\n",
    "for line in turtle_data.splitlines():\n",
    "    # If the line is a prefix, add it directly to the result\n",
    "    if line.startswith(\"@prefix\"):\n",
    "        result.append(line)\n",
    "    else:\n",
    "        # Accumulate lines for each statement\n",
    "        current_statement.append(line.strip())\n",
    "        if line.endswith(\".\"):\n",
    "            count_statements += 1\n",
    "            result.append(\" \".join(current_statement))\n",
    "            current_statement = []\n",
    "    \n",
    "    # Stop if we have reached 2 statements\n",
    "    if count_statements == 2:\n",
    "        break\n",
    "\n",
    "# Combine result lines into a string\n",
    "turtle_str = \"\\n\".join(result)\n",
    "# Split the string by the end of triple statements, and then rejoin with line returns and indentation\n",
    "# pretty_turtle_str = '.\\n'.join([triplet.strip() for triplet in turtle_str.split('. ') if triplet.strip()])\n",
    "\n",
    "# Pretty-print the Turtle string\n",
    "pretty_turtle_str = pretty_print_turtle(turtle_str)\n",
    "print(pretty_turtle_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pretty-print Turtle string with indentation and line returns\n",
    "def pretty_print_turtle(turtle_str):\n",
    "    lines = turtle_str.strip().split('\\n')\n",
    "    pretty_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if ';' in line and '.' in line:\n",
    "            triples = line.split(';')\n",
    "            formatted_triples = ' ;\\n    '.join(triple.strip() for triple in triples)\n",
    "            pretty_lines.append(formatted_triples.rstrip(' .') + ' .' )\n",
    "        else:\n",
    "            pretty_lines.append(line)\n",
    "\n",
    "    return '\\n'.join(pretty_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
