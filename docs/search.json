[
  {
    "objectID": "02_jsonld.html",
    "href": "02_jsonld.html",
    "title": "Working with JSON-LD",
    "section": "",
    "text": "We have been using the kglab tutorial to learn about using W3C web standards to construct knowledge graphs. One particularly useful standard that is briefly touched on in the tutorial is JSON-LD in the serialization exercise. This corresponds to building a knowledge graph with rdflib in the kglab/examples subdirectory. JSON-LD context\n\n\nThe beauty of JSON-LD lies in its ability to transform standard JSON data into a knowledge graph simply and efficiently. By adding a ‘(context?)’ to a JSON document, developers can define how the data should be interpreted semantically. This turns a flat data structure into an interconnected web of information, opening up powerful querying and linking possibilities.\n\n\n\nThe release of JSON-LD 1.1 introduced a range of new features aimed at enhancing its functionality and ease of use. Key updates include improved context management, support for graph containers, and the ability to nest node objects. These features make it easier to construct intricate knowledge graphs and offer more control over how data is contextualized."
  },
  {
    "objectID": "02_jsonld.html#some-notes-on-json-ld",
    "href": "02_jsonld.html#some-notes-on-json-ld",
    "title": "Working with JSON-LD",
    "section": "",
    "text": "We have been using the kglab tutorial to learn about using W3C web standards to construct knowledge graphs. One particularly useful standard that is briefly touched on in the tutorial is JSON-LD in the serialization exercise. This corresponds to building a knowledge graph with rdflib in the kglab/examples subdirectory. JSON-LD context\n\n\nThe beauty of JSON-LD lies in its ability to transform standard JSON data into a knowledge graph simply and efficiently. By adding a ‘(context?)’ to a JSON document, developers can define how the data should be interpreted semantically. This turns a flat data structure into an interconnected web of information, opening up powerful querying and linking possibilities.\n\n\n\nThe release of JSON-LD 1.1 introduced a range of new features aimed at enhancing its functionality and ease of use. Key updates include improved context management, support for graph containers, and the ability to nest node objects. These features make it easier to construct intricate knowledge graphs and offer more control over how data is contextualized."
  },
  {
    "objectID": "02_jsonld.html#json-ld-and-web-apis",
    "href": "02_jsonld.html#json-ld-and-web-apis",
    "title": "Working with JSON-LD",
    "section": "JSON-LD and Web APIs",
    "text": "JSON-LD and Web APIs\nIn the modern web ecosystem, APIs serve as the bridges between different services and applications. JSON-LD’s compatibility with web APIs makes it a top choice for developers needing to consume or provide structured, linked data. Its seamless integration with RESTful services ensures that you can work within a familiar environment while benefiting from enhanced data semantics.\nMoreover, JSON-LD’s ability to express linked data allows for more advanced operations such as data aggregation, filtering, and transformation directly via API calls. This creates opportunities for developing richer, more interactive applications that can adapt in real-time to changes in underlying data. For example, by utilizing JSON-LD in a RESTful API for a content management system, you could dynamically link related articles, authors, and tags, thereby providing a more enriched user experience.\nAdditionally, JSON-LD’s interoperability means it can be easily coupled with other web standards like OAuth for secure authentication or CORS for cross-origin resource sharing. This makes it not just a data format, but a comprehensive solution for building robust and scalable API ecosystems.\nFinally, JSON-LD also plays a significant role in the realm of Web APIs for semantic search engines and linked data platforms. These APIs can consume JSON-LD to understand the contextual relationships between different pieces of information, thereby enabling more intelligent and nuanced search queries."
  },
  {
    "objectID": "02_jsonld.html#example-exposing-json-ld-context-via-http-link-header",
    "href": "02_jsonld.html#example-exposing-json-ld-context-via-http-link-header",
    "title": "Working with JSON-LD",
    "section": "Example: Exposing JSON-LD Context via HTTP Link Header",
    "text": "Example: Exposing JSON-LD Context via HTTP Link Header\nIn many real-world applications, the JSON-LD context can be exposed via an HTTP link header, similar to how Schema.org does it. This enables clients to discover the context automatically and understand how to interpret the linked data.\nSuppose you have a RESTful API for a blog platform, and you want to expose a JSON-LD context for articles. The HTTP response could include a link header pointing to the JSON-LD context:\nHTTP/1.1 200 OK\nContent-Type: application/json\nLink: &lt;https://yourapi.com/docs/jsonldcontext.json&gt;; rel=\"http://www.w3.org/ns/json-ld#context\"; type=\"application/ld+json\"\nWith this setup, clients consuming the API can follow the link to fetch the context and understand the semantics of the data. Here is a simplified example of what the jsonldcontext.json might look like:\n{\n  \"@context\": {\n    \"title\": \"http://schema.org/headline\",\n    \"author\": \"http://schema.org/author\",\n    \"datePublished\": \"http://schema.org/datePublished\",\n    \"content\": \"http://schema.org/text\"\n  }\n}\nBy using the link header to expose the JSON-LD context, you’re making it easier for clients to consume and understand your API’s data. This aligns well with JSON-LD 1.1 conventions and allows for greater interoperability and semantic richness."
  },
  {
    "objectID": "02_jsonld.html#example-using-json-ld-to-construct-a-knowledge-graph-from-wikipedia-tables",
    "href": "02_jsonld.html#example-using-json-ld-to-construct-a-knowledge-graph-from-wikipedia-tables",
    "title": "Working with JSON-LD",
    "section": "Example: Using JSON-LD to construct a Knowledge Graph from Wikipedia tables",
    "text": "Example: Using JSON-LD to construct a Knowledge Graph from Wikipedia tables\nWikipedia can provide a starting point for structuring and enriching knowledge graphs. For our Navy project, we will want to provide LLM Agents with context for factual information that may have changed since “pre-training” using Retrieval Augmented Generation. For our purposes, we want to augment our KG with broader contextual information that may be useful to agents for question and answering. One set of context, is the List of current ships of the United States Navy that contain a number of tables that could be useful to add to our knowledge graph. Wikidata does contain most of the ships but the entities are not always complete. For example, the Wikidata entity page for USS Abraham Lincoln shows a rather rich set of relations including events that correspond to ship naming ceremony, ship commissioning. However, ships like USS Carter Hall are very basic and don’t contain a lot of information we can extract. Looking at the History for List of current ships of the United States Navy tells us that the page is fairly active and Wikipedia contributors are keeping it more up to date than the Wikidata entries.\nLets’ do a little Exploratory Data Analysis using large language models similar to Getting Started With LLMs. We will use Beautiful Soup to retreive the Wikipedia page and find the “html tables” in the document.\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl = \"https://en.wikipedia.org/wiki/List_of_current_ships_of_the_United_States_Navy\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\ntable = soup.find_all(\"table\")[0]\n\n\nNow that we have the tables, lets create a pandas dataframe from the table and sanity check that the table is the same as what is in the first table.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_html(str(table))[0]\ndf\n\n\n/tmp/ipykernel_4540/553751003.py:3: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table))[0]\n\n\n\n\n\n\n\n\n\n\nShip name\nHull number\nClass\nType\nCommission date\nHomeport[2]\nNote\n\n\n\n\n0\nUSS Abraham Lincoln\nCVN-72\nNimitz\nAircraft carrier\n11 November 1989\nSan Diego, CA\n[3]\n\n\n1\nUSS Alabama\nSSBN-731\nOhio\nBallistic missile submarine\n25 May 1985\nBangor, WA\n[4]\n\n\n2\nUSS Alaska\nSSBN-732\nOhio\nBallistic missile submarine\n25 January 1986\nKings Bay, GA\n[5]\n\n\n3\nUSS Albany\nSSN-753\nLos Angeles\nAttack submarine\n7 April 1990\nNorfolk, VA\n[6]\n\n\n4\nUSS Alexandria\nSSN-757\nLos Angeles\nAttack submarine\n29 June 1991\nSan Diego, CA\n[7] Scheduled to be decommissioned 2026[8]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n234\nUSS William P. Lawrence\nDDG-110\nArleigh Burke\nDestroyer\n19 May 2011\nSan Diego, CA\n[242]\n\n\n235\nUSS Winston S. Churchill\nDDG-81\nArleigh Burke\nDestroyer\n10 March 2001\nNorfolk, VA\n[243]\n\n\n236\nUSS Wichita\nLCS-13\nFreedom\nLittoral combat ship\n12 January 2019\nMayport, FL\n[244] Proposed to be decommissioned 2023[17]\n\n\n237\nUSS Wyoming\nSSBN-742\nOhio\nBallistic missile submarine\n13 July 1996\nKings Bay, GA\n[245]\n\n\n238\nUSS Zumwalt\nDDG-1000\nZumwalt\nDestroyer\n15 October 2016\nSan Diego, CA\n[246]\n\n\n\n\n239 rows × 7 columns\n\nFigure 1: Dataframe from Wikipedia Commissioned ship table.\n\n\n\nSo, if we want to use the column names as the basis for eventually constructing a URI, we unfortunately need to make it web safe and remove spaces and other issues. The other question is “what does a row” represent? If we want to build a knowledge graph of all of the ships of the navy, we might want to consider a basic ontology to start with. The first table is “commissioned” ships and the idea of commisioned is a role relationship since it has a start time and a end time.\n\n\nCode\ncolumn_names = df.columns.tolist()\ncolumn_names\n\n\n['Ship name',\n 'Hull number',\n 'Class',\n 'Type',\n 'Commission date',\n 'Homeport[2]',\n 'Note']\n\n\n\n\n\nCode\nimport json\nimport uuid\n\nfrom pprint import pprint\n\n# Normalize column names in DataFrame\nnormalized_columns = {col: col.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\") for col in column_names}\ndf.rename(columns=normalized_columns, inplace=True)\n\n# Let's change the date to be consistent with xsd:date\n# df['Commission_date'] = pd.to_datetime(df['Commission_date']).dt.strftime('%Y-%m-%d')\n\n# Convert DataFrame to JSON-formatted string\ndf_json_str = df.to_json(orient=\"records\", indent=4)\n\n# Convert JSON-formatted string to Python object\ndf_json = json.loads(df_json_str)\n\n# Pretty-print the first record\npprint(df_json[0])\n\n\n{'Class': 'Nimitz',\n 'Commission_date': '11 November 1989',\n 'Homeport2': 'San Diego, CA',\n 'Hull_number': 'CVN-72',\n 'Note': '[3]',\n 'Ship_name': 'USS\\xa0Abraham Lincoln',\n 'Type': 'Aircraft carrier'}\n\nFigure 2: ?(caption)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "Welcome to FastKG-Course, an interactive course designed to make you proficient in Knowledge Graphs, Semantic Technologies, Ontology, and more. This project is affiliated with the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame.\n\n\n\n\nInteractive Jupyter Notebooks\nHands-on Tutorials\nUse of fast.ai and kglab libraries\n\n\n\n\n\n\n\n\n\n\n(Optional) Fork this repository\nOpen the project in GitHub Codespaces\nThe devcontainer will automatically set up your environment\nStart the Jupyter server and open the introductory notebook\n\n\n\n\nBefore starting this course, it is highly recommended to complete the following Fast.ai courses:\n\nPractical Deep Learning for Coders\nDeep Learning for Coders with fastai & Pytorch\nJupyter Notebook 101\n\nAdditional requirements:\n\nBasic knowledge of Python\nFamiliarity with machine learning concepts\n\n\n\n\nNavigate through the Jupyter notebooks in sequence for a curated educational path, or feel free to explore topics that interest you.\nThe dev container is fully configured with software and KG and machine learning libraries needed for this course.\n\n\n\n\nCharles F Vardeman II\n\n\n\n\nThis project is part of the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame. The kglab project for providing some integrated tools for working with Knowledge Graphs as well as an excellent tutorial. Content generation and assistance have been facilitated using OpenAI’s GPT-4 language model. ChatGPT August 3, 2023 version\n\n\n\nThis project is dual-licensed under:\n\nMIT License for the software components. See LICENSE-MIT for more details.\nCreative Commons Attribution 4.0 International (CC BY 4.0) for the educational content. See LICENSE-CC-BY for more details.\nThe kglab submodule from derwen.ai is licensed under the MIT License.\n\nYou are free to use the project under either of the licenses, depending on your needs."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "Welcome to FastKG-Course, an interactive course designed to make you proficient in Knowledge Graphs, Semantic Technologies, Ontology, and more. This project is affiliated with the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "Interactive Jupyter Notebooks\nHands-on Tutorials\nUse of fast.ai and kglab libraries"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "(Optional) Fork this repository\nOpen the project in GitHub Codespaces\nThe devcontainer will automatically set up your environment\nStart the Jupyter server and open the introductory notebook"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "Before starting this course, it is highly recommended to complete the following Fast.ai courses:\n\nPractical Deep Learning for Coders\nDeep Learning for Coders with fastai & Pytorch\nJupyter Notebook 101\n\nAdditional requirements:\n\nBasic knowledge of Python\nFamiliarity with machine learning concepts"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "Navigate through the Jupyter notebooks in sequence for a curated educational path, or feel free to explore topics that interest you.\nThe dev container is fully configured with software and KG and machine learning libraries needed for this course."
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "Charles F Vardeman II"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "This project is part of the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame. The kglab project for providing some integrated tools for working with Knowledge Graphs as well as an excellent tutorial. Content generation and assistance have been facilitated using OpenAI’s GPT-4 language model. ChatGPT August 3, 2023 version"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "A “Fast” Introduction to Knowledge Graphs",
    "section": "",
    "text": "This project is dual-licensed under:\n\nMIT License for the software components. See LICENSE-MIT for more details.\nCreative Commons Attribution 4.0 International (CC BY 4.0) for the educational content. See LICENSE-CC-BY for more details.\nThe kglab submodule from derwen.ai is licensed under the MIT License.\n\nYou are free to use the project under either of the licenses, depending on your needs."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This set of jupyter notebooks and codespace are meant to bootstrap your understanding of Knowledge Engineering, Knowledge Graphs, Linked Data and how Generative AI can leverage these technologies to be more factual. We say a Fast start because we want to use the learning principles as espoused by the fast.ai - fast.ai—Making neural nets uncool again and summarized in the blog fast.ai - Providing a Good Education in Deep Learning. Hopefully this will make Knowledge Engineering just as uncool as neural networks.\n\n\nGiven that we are building on the Fast.ai approach, we will leverage some of their introductory materials for doing machine learning and working with AI. At a minimum, the readers should be familiar with:\n\nPractical Deep Learning for Coders - 1: Getting started (fast.ai)\ncourse22/01-jupyter-notebook-101.ipynb at master · fastai/course22 (github.com)\nPractical Deep Learning for Coders - 2: Deployment (fast.ai)\nPractical Deep Learning for Coders - 3: Neural net foundations (fast.ai)\nPractical Deep Learning for Coders - 4: Natural Language (NLP) (fast.ai)\n\n\n\n\n\n\n\nUse of Generative AI for learning!\n\n\n\nIt turns out that ChatGPT, Bing Chat, Bard, Claude v2 and other Generative AI Agents tuned for Conversational AI have a decent knowledge of the “Semantic Web Stack” and are particularly good at using Python tools for the semantic web like rdflib. We highly recommend using these copilots for creating code blocks, debugging and solving problems. We will leverage these agents in later notebooks for assisting in “ontology” construction.\n\n\n\n\n\nA vision for a semantic web was laid out by the “father” of the World Wide Web Tim Berners-Lee and his co-authors (Berners-Lee, Hendler, and Lassila 2001) in the paper “The Semantic Web” that involved Software Agents, Shared Formal Concepts called Ontologies, and Linked Data to provide the structure for a world driven by Assistive Agents. To further this vision, the standards body for the Web, the World Wide Web Consortium (W3C) created standards for exchanging semantic Knowledge Graphs called the Resource Description Framework (RDF) and a method for specifying formal shared concepts called Web Ontology Language (OWL). The publication, “A data engineer’s guide to semantic modeling” direct pdf download by Ilaria Maresi (Maresi 2020) of The Hyve provides a very accessible guide to “semantic modeling” and will be the primary reference going forward.\n\nBerners-Lee, Tim, James Hendler, and Ora Lassila. 2001. “The Semantic Web.” Scientific American 284 (5): 34–43. https://www.jstor.org/stable/26059207.\n\nMaresi, Ilaria. 2020. A Data Engineer’s Guide to Semantic Modelling. Zenodo. https://doi.org/10.5281/zenodo.3898519."
  },
  {
    "objectID": "01_intro.html#a-fast-start-to-knowledge-graphs-using-semantic-technologies",
    "href": "01_intro.html#a-fast-start-to-knowledge-graphs-using-semantic-technologies",
    "title": "Introduction",
    "section": "",
    "text": "This set of jupyter notebooks and codespace are meant to bootstrap your understanding of Knowledge Engineering, Knowledge Graphs, Linked Data and how Generative AI can leverage these technologies to be more factual. We say a Fast start because we want to use the learning principles as espoused by the fast.ai - fast.ai—Making neural nets uncool again and summarized in the blog fast.ai - Providing a Good Education in Deep Learning. Hopefully this will make Knowledge Engineering just as uncool as neural networks.\n\n\nGiven that we are building on the Fast.ai approach, we will leverage some of their introductory materials for doing machine learning and working with AI. At a minimum, the readers should be familiar with:\n\nPractical Deep Learning for Coders - 1: Getting started (fast.ai)\ncourse22/01-jupyter-notebook-101.ipynb at master · fastai/course22 (github.com)\nPractical Deep Learning for Coders - 2: Deployment (fast.ai)\nPractical Deep Learning for Coders - 3: Neural net foundations (fast.ai)\nPractical Deep Learning for Coders - 4: Natural Language (NLP) (fast.ai)\n\n\n\n\n\n\n\nUse of Generative AI for learning!\n\n\n\nIt turns out that ChatGPT, Bing Chat, Bard, Claude v2 and other Generative AI Agents tuned for Conversational AI have a decent knowledge of the “Semantic Web Stack” and are particularly good at using Python tools for the semantic web like rdflib. We highly recommend using these copilots for creating code blocks, debugging and solving problems. We will leverage these agents in later notebooks for assisting in “ontology” construction.\n\n\n\n\n\nA vision for a semantic web was laid out by the “father” of the World Wide Web Tim Berners-Lee and his co-authors (Berners-Lee, Hendler, and Lassila 2001) in the paper “The Semantic Web” that involved Software Agents, Shared Formal Concepts called Ontologies, and Linked Data to provide the structure for a world driven by Assistive Agents. To further this vision, the standards body for the Web, the World Wide Web Consortium (W3C) created standards for exchanging semantic Knowledge Graphs called the Resource Description Framework (RDF) and a method for specifying formal shared concepts called Web Ontology Language (OWL). The publication, “A data engineer’s guide to semantic modeling” direct pdf download by Ilaria Maresi (Maresi 2020) of The Hyve provides a very accessible guide to “semantic modeling” and will be the primary reference going forward.\n\nBerners-Lee, Tim, James Hendler, and Ora Lassila. 2001. “The Semantic Web.” Scientific American 284 (5): 34–43. https://www.jstor.org/stable/26059207.\n\nMaresi, Ilaria. 2020. A Data Engineer’s Guide to Semantic Modelling. Zenodo. https://doi.org/10.5281/zenodo.3898519."
  },
  {
    "objectID": "01_intro.html#knowledge-graphs",
    "href": "01_intro.html#knowledge-graphs",
    "title": "Introduction",
    "section": "Knowledge Graphs",
    "text": "Knowledge Graphs\nA more technical reference on the topic of Knowledge Graphs is provided in the book (Hogan et al. 2021) and is the recommended definitive concise reference on Knowledge Graphs. The HTML version is available online and print versions are published by Morgan & Claypool Publishers. One should be familiar with:\n\nHogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, et al. 2021. Knowledge Graphs. Synthesis Lectures on Data, Semantics, and Knowledge 22. Springer. https://doi.org/10.2200/S01125ED1V01Y202109DSK022.\n\nChapter 1. Introduction\nChapter 2. Data Graphs\nChapter 6. Creation and Enrichment\nChapter 7. Publication\n\nAdditionally, the following chapters are useful for the kglab tutorial and highly recommended:\n\nChapter 3. Schema, Identity, Context\nChapter 4. Deductive Knowledge\n\n\n\n\n\n\n\nNote\n\n\n\nWe use the “Knowledge Graphs” books definition of a knowledge graph from the introduction:\n\n“… as a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent relations between these entities.”\n\n\n\n\nTools for constructing knowledge graphs\nDerwen.ai has created a Python Package called kglab that “provides a simple abstraction layer in Python for building knowledge graphs.” The kglab tutorial is quite good at providing an introduction to the semantic web stack and have included the GitHub repository as a git submodule.\nWe particularly recommend working through:\n\nData and Ontology sources - kglab (derwen.ai) in kglab/examples/ex0_0.pynb\nBuild an RDF graph using RDFlib - kglab (derwen.ai) in kglab/examples/ex1_0.pynb\nLeverage the kglab abstraction layer - kglab (derwen.ai) in in kglab/examples/ex1_1.pynb\nBuild a medium size KG from a CSV dataset - kglab (derwen.ai) in in kglab/examples/ex2_0.pynb\nRun SPARQL queries - kglab (derwen.ai) in kglab/examples/ex4_0.pynb\nInteractive graph visualization with PyVis - kglab (derwen.ai) in in kglab/examples/ex3_0.pynb\nMeasurement and inference - kglab (derwen.ai) in kglab/examples/ex7_0.pynb\n\nThe Devcontainer in the code space should automatically install from the .devcontainer/requirements.txt should install fastai, kglab and pandas needed for other parts of this tutorial. To check these are installed correctly, the following code block\n\n# Import libraries\nimport fastai\nimport kglab\nimport pandas\n\n# Print versions\nprint(f'fastai version: {fastai.__version__}')\nprint(f'kglab version: {kglab.__version__}')\nprint(f'pandas version: {pandas.__version__}')\n\nfastai version: 2.7.12\nkglab version: 0.6.6\npandas version: 2.1.0"
  }
]