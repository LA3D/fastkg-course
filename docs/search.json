[
  {
    "objectID": "03_webapi.html",
    "href": "03_webapi.html",
    "title": "JSON-LD and WebAPIs",
    "section": "",
    "text": "In the modern web ecosystem, APIs serve as the bridges between different services and applications. JSON-LD’s compatibility with web APIs makes it a top choice for developers needing to consume or provide structured, linked data. Its seamless integration with RESTful services ensures that you can work within a familiar environment while benefiting from enhanced data semantics.\nMoreover, JSON-LD’s ability to express linked data allows for more advanced operations such as data aggregation, filtering, and transformation directly via API calls. This creates opportunities for developing richer, more interactive applications that can adapt in real-time to changes in underlying data. For example, by utilizing JSON-LD in a RESTful API for a content management system, you could dynamically link related articles, authors, and tags, thereby providing a more enriched user experience.\nAdditionally, JSON-LD’s interoperability means it can be easily coupled with other web standards like OAuth for secure authentication or CORS for cross-origin resource sharing. This makes it not just a data format, but a comprehensive solution for building robust and scalable API ecosystems.\nFinally, JSON-LD also plays a significant role in the realm of Web APIs for semantic search engines and linked data platforms. These APIs can consume JSON-LD to understand the contextual relationships between different pieces of information, thereby enabling more intelligent and nuanced search queries."
  },
  {
    "objectID": "03_webapi.html#json-ld-and-web-apis",
    "href": "03_webapi.html#json-ld-and-web-apis",
    "title": "JSON-LD and WebAPIs",
    "section": "",
    "text": "In the modern web ecosystem, APIs serve as the bridges between different services and applications. JSON-LD’s compatibility with web APIs makes it a top choice for developers needing to consume or provide structured, linked data. Its seamless integration with RESTful services ensures that you can work within a familiar environment while benefiting from enhanced data semantics.\nMoreover, JSON-LD’s ability to express linked data allows for more advanced operations such as data aggregation, filtering, and transformation directly via API calls. This creates opportunities for developing richer, more interactive applications that can adapt in real-time to changes in underlying data. For example, by utilizing JSON-LD in a RESTful API for a content management system, you could dynamically link related articles, authors, and tags, thereby providing a more enriched user experience.\nAdditionally, JSON-LD’s interoperability means it can be easily coupled with other web standards like OAuth for secure authentication or CORS for cross-origin resource sharing. This makes it not just a data format, but a comprehensive solution for building robust and scalable API ecosystems.\nFinally, JSON-LD also plays a significant role in the realm of Web APIs for semantic search engines and linked data platforms. These APIs can consume JSON-LD to understand the contextual relationships between different pieces of information, thereby enabling more intelligent and nuanced search queries."
  },
  {
    "objectID": "03_webapi.html#example-exposing-json-ld-context-via-http-link-header",
    "href": "03_webapi.html#example-exposing-json-ld-context-via-http-link-header",
    "title": "JSON-LD and WebAPIs",
    "section": "Example: Exposing JSON-LD Context via HTTP Link Header",
    "text": "Example: Exposing JSON-LD Context via HTTP Link Header\nIn many real-world applications, the JSON-LD context can be exposed via an HTTP link header, similar to how Schema.org does it. This enables clients to discover the context automatically and understand how to interpret the linked data.\nSuppose you have a RESTful API for a blog platform, and you want to expose a JSON-LD context for articles. The HTTP response could include a link header pointing to the JSON-LD context:\nHTTP/1.1 200 OK\nContent-Type: application/json\nLink: &lt;https://yourapi.com/docs/jsonldcontext.json&gt;; rel=\"http://www.w3.org/ns/json-ld#context\"; type=\"application/ld+json\"\nWith this setup, clients consuming the API can follow the link to fetch the context and understand the semantics of the data. Here is a simplified example of what the jsonldcontext.json might look like:\n{\n  \"@context\": {\n    \"title\": \"http://schema.org/headline\",\n    \"author\": \"http://schema.org/author\",\n    \"datePublished\": \"http://schema.org/datePublished\",\n    \"content\": \"http://schema.org/text\"\n  }\n}\nBy using the link header to expose the JSON-LD context, you’re making it easier for clients to consume and understand your API’s data. This aligns well with JSON-LD 1.1 conventions and allows for greater interoperability and semantic richness."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "Quickstart to Knowledge Graphs with Semantic Technologies: A fast.ai Approach",
    "section": "",
    "text": "Welcome to this hands-on tutorial, engineered to accelerate your grasp of Knowledge Engineering, Knowledge Graphs, Linked Data, and the role of Generative AI in factual enrichment. We adopt the pedagogical philosophy of fast.ai, as articulated in their blog post Providing a Good Education in Deep Learning, to make the realm of Knowledge Engineering as approachable as neural networks have become.\n\nWhy the fast.ai Approach?\nThe fast.ai philosophy emphasizes learning by doing, with a top-down approach that introduces complex topics through practical examples and projects. This hands-on, application-first methodology allows for quick, tangible results that not only bolster understanding but also demonstrate the utility of the subject matter. By incorporating these principles, we aim to demystify the seemingly arcane field of Knowledge Engineering and to deliver both the ‘why’ and the ‘how’ in a manner that encourages proactive learning.\n\n\nPrerequisites\nTo harmonize with the fast.ai approach, this tutorial leverages some of their key introductory materials. Familiarity with the following topics is strongly recommended:\n\nGetting Started with Practical Deep Learning\nJupyter Notebook 101\nDeep Learning Deployment\nNeural Network Foundations\nNatural Language Processing\n\n\n\n\n\n\n\nGenerative AI as Learning Companions\n\n\n\nGenerative AI agents like ChatGPT, Bing Chat, and Bard offer proficiency in Semantic Web technologies and Python tools like rdflib. They serve as invaluable ‘copilots’ for code generation, debugging, and problem-solving and will be incorporated into later notebooks for ontology construction. Generative AI can also assist with revising text in markdown cells serving as an editor for content.\n\n\n\n\nThe Legacy of The Semantic Web\nThe vision for a semantic web was first articulated by Tim Berners-Lee and his co-authors in their landmark paper “The Semantic Web” published in Scientific American (Berners-Lee, Hendler, and Lassila 2001). This pioneering work envisaged a web ecosystem where Software Agents, Shared Ontologies, and Linked Data would serve as the backbone for assistive agents and intelligent systems. Our tutorial aligns with this vision and the standards that have evolved from it, such as RDF and OWL.\n\nBerners-Lee, Tim, James Hendler, and Ora Lassila. 2001. “The Semantic Web.” Scientific American 284 (5): 34–43. https://www.jstor.org/stable/26059207.\n\n\nFurther Reading\nFor an in-depth technical understanding, we recommend “A data engineer’s guide to semantic modeling” by Ilaria Maresi (@ Maresi 2020) as key references.\n\nMaresi, Ilaria. 2020. A Data Engineer’s Guide to Semantic Modelling. Zenodo. https://doi.org/10.5281/zenodo.3898519.\n\n\nKnowledge Graphs: What You Need to Know\nFor a more technical understanding, the book “Knowledge Graphs” (Hogan et al. 2021) offers a definitive guide. Key chapters to review include:\n\nHogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, et al. 2021. Knowledge Graphs. Synthesis Lectures on Data, Semantics, and Knowledge 22. Springer. https://doi.org/10.2200/S01125ED1V01Y202109DSK022.\n\nIntroduction, Data Graphs, Creation and Enrichment, and Publication\n\nAdditional recommended chapters include Schema, Identity, Context and Deductive Knowledge.\n\n\n\n\n\n\nNote\n\n\n\nWe use the “Knowledge Graph” books definition of a knowledge graph from the introduction:\n\n“… as a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent relations between these entities.”\n\nThis definition is broad enough to include embedding structured Knowledge Graph databases like KùzuDB and Retrieval Augmented Generation (RAG) in frameworks like LlamaIndex that can automate LLM based query retrieval using knowledge graphs with Large Language Models.\n\n\n\n\nTooling Up\nDerwen.ai’s Python package kglab will serve as our primary toolset for building knowledge graphs. The kglab tutorial is a complementary resource worth exploring. We have included the tutorial as a [Git Submodule](https://github.blog/2016-02-01-working-with-submodules/ in the kglab directory of this repository.\nWe invite you to immerse yourself in the provided examples for hands-on practice and a more comprehensive understanding of knowledge graphs.\n\nData and Ontology sources - kglab (derwen.ai) in kglab/examples/ex0_0.pynb\nBuild an RDF graph using RDFlib - kglab (derwen.ai) in kglab/examples/ex1_0.pynb\nLeverage the kglab abstraction layer - kglab (derwen.ai) in in kglab/examples/ex1_1.pynb\nBuild a medium size KG from a CSV dataset - kglab (derwen.ai) in in kglab/examples/ex2_0.pynb\nRun SPARQL queries - kglab (derwen.ai) in kglab/examples/ex4_0.pynb\nInteractive graph visualization with PyVis - kglab (derwen.ai) in in kglab/examples/ex3_0.pynb\nMeasurement and inference - kglab (derwen.ai) in kglab/examples/ex7_0.pynb\n\nThe Devcontainer in the code space should automatically install from the .devcontainer/requirements.txt fastai, kglab and pandas needed for other parts of this tutorial. To check these are installed correctly, the following code block\n\n# Import libraries\nimport fastai\nimport kglab\nimport pandas\n\n# Print versions\nprint(f'fastai version: {fastai.__version__}')\nprint(f'kglab version: {kglab.__version__}')\nprint(f'pandas version: {pandas.__version__}')\n\nfastai version: 2.7.12\nkglab version: 0.6.6\npandas version: 2.1.0"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Knowledge Graphs",
    "section": "Introduction",
    "text": "Introduction\nWelcome to FastKG-Course, an interactive course designed to make you proficient in Knowledge Graphs, Semantic Technologies, Ontology, and more. This project is affiliated with the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Knowledge Graphs",
    "section": "Features",
    "text": "Features\n\nInteractive Jupyter Notebooks\nHands-on Tutorials\nUse of fast.ai and kglab libraries"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Knowledge Graphs",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\n(Optional) Fork this repository\nOpen the project in GitHub Codespaces\nThe devcontainer will automatically set up your environment\nStart the Jupyter server and open the introductory notebook"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Knowledge Graphs",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this course, it is highly recommended to complete the following Fast.ai courses:\n\nPractical Deep Learning for Coders\nDeep Learning for Coders with fastai & Pytorch\nJupyter Notebook 101\n\nAdditional requirements:\n\nBasic knowledge of Python\nFamiliarity with machine learning concepts"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Knowledge Graphs",
    "section": "Usage",
    "text": "Usage\nNavigate through the Jupyter notebooks in sequence for a curated educational path, or feel free to explore topics that interest you.\nThe dev container is fully configured with software and KG and machine learning libraries needed for this course."
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Knowledge Graphs",
    "section": "Contributors",
    "text": "Contributors\n\nCharles F Vardeman II"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Knowledge Graphs",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis project is part of the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame. The kglab project for providing some integrated tools for working with Knowledge Graphs as well as an excellent tutorial. Content generation and assistance have been facilitated using OpenAI’s GPT-4 language model. ChatGPT August 3, 2023 version"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Knowledge Graphs",
    "section": "License",
    "text": "License\nThis project is dual-licensed under:\n\nMIT License for the software components. See LICENSE-MIT for more details.\nCreative Commons Attribution 4.0 International (CC BY 4.0) for the educational content. See LICENSE-CC-BY for more details.\nThe kglab submodule from derwen.ai is licensed under the MIT License.\n\nYou are free to use the project under either of the licenses, depending on your needs."
  },
  {
    "objectID": "02_jsonld.html",
    "href": "02_jsonld.html",
    "title": "Working with JSON-LD",
    "section": "",
    "text": "We have been using the kglab tutorial to learn about using W3C web standards to construct knowledge graphs. One particularly useful standard that is briefly touched on in the tutorial is JSON-LD in the serialization exercise. This corresponds to building a knowledge graph with rdflib in the kglab/examples subdirectory."
  },
  {
    "objectID": "02_jsonld.html#uning-json-ld-and-generative-ai-to-construct-a-knowledge-graph",
    "href": "02_jsonld.html#uning-json-ld-and-generative-ai-to-construct-a-knowledge-graph",
    "title": "Working with JSON-LD",
    "section": "",
    "text": "We have been using the kglab tutorial to learn about using W3C web standards to construct knowledge graphs. One particularly useful standard that is briefly touched on in the tutorial is JSON-LD in the serialization exercise. This corresponds to building a knowledge graph with rdflib in the kglab/examples subdirectory."
  },
  {
    "objectID": "02_jsonld.html#using-generative-ai-to-create-column-types.",
    "href": "02_jsonld.html#using-generative-ai-to-create-column-types.",
    "title": "Working with JSON-LD",
    "section": "Using Generative AI to create column types.",
    "text": "Using Generative AI to create column types.\n(Korini and Bizer 2023)\n\nKorini, Keti, and Christian Bizer. 2023. “Column Type Annotation Using ChatGPT.” July 30, 2023. http://arxiv.org/abs/2306.00745.\n\n\n\n\n\n\nChatGPT Advanced Data Analysis:\n\n\nAbsolutely, having a summary and a set of steps can make the process easily reproducible. Here it is:\n\n\n## Summary\n\n\nTo generate Turtle data from a Pandas DataFrame:\n\n\n1. Pandas to JSON: Convert the DataFrame to JSON format using to_json() with orient='records'.\n\n\n2. Add UUID and Type to JSON: Iterate through the JSON objects and add a UUID prefixed by urn:uuid: and a @type with the value Person.\n\n\n3. Create JSON-LD Context: Define a JSON-LD context to map JSON keys to RDF vocabularies. Include mappings for the UUID and rdf:type.\n\n\n4. Combine to Form JSON-LD: Create a JSON-LD object that incorporates both the JSON data and the JSON-LD context. Put the JSON objects inside a @graph array.\n\n\n5. Load into RDFlib: Use RDFlib’s parse() method to load the JSON-LD data into an RDF graph.\n\n\n6. Serialize to Turtle: Use RDFlib’s serialize() method to convert the RDF graph to Turtle format.\n\n\n## Code Prompt\n\n\nHere’s a condensed Python code snippet that includes all the steps:\n\n\n```python import pandas as pd import json import uuid from rdflib import Graph\n\n\n# Step 1: Convert DataFrame to JSON data = {‘Name’: [‘Alice’, ‘Bob’], ‘Age’: [25, 30], ‘Occupation’: [‘Engineer’, ‘Doctor’]} df = pd.DataFrame(data) json_data = df.to_json(orient=‘records’)\n\n\n# Step 2: Add UUID and Type to JSON json_data_dict = json.loads(json_data) for obj in json_data_dict: obj[“UUID”] = “urn:uuid:” + str(uuid.uuid4()) obj[“(type?)”] = “Person”\n\n\n# Step 3: Create JSON-LD Context context = { “(context?)”: { “UUID”: “(id?)”, “Name”: “http://schema.org/name”, “Age”: “http://schema.org/age”, “Occupation”: “http://schema.org/occupation”, “(type?)”: “http://www.w3.org/1999/02/22-rdf-syntax-ns#type”, “Person”: “http://xmlns.com/foaf/0.1/Person” } }\n\n\n# Step 4: Combine to Form JSON-LD json_ld_data = { “(context?)”: context[“(context?)”], “(graph?)”: json_data_dict }\n\n\n# Step 5: Load into RDFlib g = Graph() g.parse(data=json.dumps(json_ld_data), format=‘json-ld’)\n\n\n# Step 6: Serialize to Turtle turtle_data = g.serialize(format=‘turtle’) print(turtle_data) ```\n\n\nThis will generate Turtle data that includes UUIDs and a rdf:type of Person, aligning with your requirements.\n\n\n\n\nTransforming Standard JSON to a Knowledge Graph\nThe beauty of JSON-LD lies in its ability to transform standard JSON data into a knowledge graph simply and efficiently. By adding a ‘(context?)’ to a JSON document, developers can define how the data should be interpreted semantically. This turns a flat data structure into an interconnected web of information, opening up powerful querying and linking possibilities."
  },
  {
    "objectID": "02_jsonld.html#our-general-procedure-towards-automated-kg-construction-using-llms",
    "href": "02_jsonld.html#our-general-procedure-towards-automated-kg-construction-using-llms",
    "title": "Working with JSON-LD",
    "section": "Our general procedure towards automated KG Construction using LLMs",
    "text": "Our general procedure towards automated KG Construction using LLMs\nI understand the workflow you’ve outlined for constructing a knowledge graph from CSV data using JSON-LD. The process includes six major steps:\n\nETL from CSV to JSON: Extract, transform, and load the data from a CSV file into a JSON object. Here, we’ll use Python libraries like pandas.\nCreating JSON-LD Vocabulary File: Establish the terms, data types, and definitions in a JSON-LD vocabulary file based on the LLM template.\nConvert from JSON to JSON-LD: Extend the JSON data to JSON-LD by adding the context, linking it with the vocabulary’s IRI, wrapping the data in a (graph?) object, and assigning unique IDs.\nIntroduce New Ontology File: Create another JSON-LD vocabulary file to serve as an ontology.\nLink Vocabularies in Another JSON-LD File: Establish relationships between the vocabularies, definitions, and ontologies in a separate JSON-LD file.\nCapturing Provenance Information: Document metadata about the (graph?) object to capture its origin and transformation history."
  },
  {
    "objectID": "02_jsonld.html#example-using-json-ld-to-construct-a-knowledge-graph-from-wikipedia-tables",
    "href": "02_jsonld.html#example-using-json-ld-to-construct-a-knowledge-graph-from-wikipedia-tables",
    "title": "Working with JSON-LD",
    "section": "Example: Using JSON-LD to construct a Knowledge Graph from Wikipedia tables",
    "text": "Example: Using JSON-LD to construct a Knowledge Graph from Wikipedia tables\nWikipedia can provide a starting point for structuring and enriching knowledge graphs. For our Navy project, we will want to provide LLM Agents with context for factual information that may have changed since “pre-training” using Retrieval Augmented Generation. For our purposes, we want to augment our KG with broader contextual information that may be useful to agents for question and answering. One set of context, is the List of current ships of the United States Navy that contain a number of tables that could be useful to add to our knowledge graph. Wikidata does contain most of the ships but the entities are not always complete. For example, the Wikidata entity page for USS Abraham Lincoln shows a rather rich set of relations including events that correspond to ship naming ceremony, ship commissioning. However, ships like USS Carter Hall are very basic and don’t contain a lot of information we can extract. Looking at the History for List of current ships of the United States Navy tells us that the page is fairly active and Wikipedia contributors are keeping it more up to date than the Wikidata entries.\nLets’ do a little Exploratory Data Analysis using large language models similar to Getting Started With LLMs. We will use Beautiful Soup to retreive the Wikipedia page and find the “html tables” in the document.\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl = \"https://en.wikipedia.org/wiki/List_of_current_ships_of_the_United_States_Navy\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\ntable = soup.find_all(\"table\")[0]\n\n\nNow that we have the tables, lets create a pandas dataframe from the table and sanity check that the table is the same as what is in the first table.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_html(str(table))[0]\ndf\n\n\n/tmp/ipykernel_43085/3411228357.py:7: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table))[0]\n\n\n\n\n\n\n\n\n\n\nShip name\nHull number\nClass\nType\nCommission date\nHomeport[2]\nNote\n\n\n\n\n0\nUSS Abraham Lincoln\nCVN-72\nNimitz\nAircraft carrier\n11 November 1989\nSan Diego, CA\n[3]\n\n\n1\nUSS Alabama\nSSBN-731\nOhio\nBallistic missile submarine\n25 May 1985\nBangor, WA\n[4]\n\n\n2\nUSS Alaska\nSSBN-732\nOhio\nBallistic missile submarine\n25 January 1986\nKings Bay, GA\n[5]\n\n\n3\nUSS Albany\nSSN-753\nLos Angeles\nAttack submarine\n7 April 1990\nNorfolk, VA\n[6]\n\n\n4\nUSS Alexandria\nSSN-757\nLos Angeles\nAttack submarine\n29 June 1991\nSan Diego, CA\n[7] Scheduled to be decommissioned 2026[8]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n233\nUSS William P. Lawrence\nDDG-110\nArleigh Burke\nDestroyer\n19 May 2011\nSan Diego, CA\n[241]\n\n\n234\nUSS Winston S. Churchill\nDDG-81\nArleigh Burke\nDestroyer\n10 March 2001\nNorfolk, VA\n[242]\n\n\n235\nUSS Wichita\nLCS-13\nFreedom\nLittoral combat ship\n12 January 2019\nMayport, FL\n[243] Proposed to be decommissioned 2023[17]\n\n\n236\nUSS Wyoming\nSSBN-742\nOhio\nBallistic missile submarine\n13 July 1996\nKings Bay, GA\n[244]\n\n\n237\nUSS Zumwalt\nDDG-1000\nZumwalt\nDestroyer\n15 October 2016\nSan Diego, CA\n[245]\n\n\n\n\n238 rows × 7 columns\n\nFigure 1: Dataframe from Wikipedia Commissioned ship table.\n\n\n\nSo, if we want to use the column names as the basis for eventually constructing a URI, we unfortunately need to make it web safe and remove spaces and other issues. The other question is “what does a row” represent? If we want to build a knowledge graph of all of the ships of the navy, we might want to consider a basic ontology to start with. The first table is “commissioned” ships and the idea of commisioned is a role relationship since it has a start time and a end time.\n\n\nCode\ncolumn_names = df.columns.tolist()\ncolumn_names\n\n\n['Ship name',\n 'Hull number',\n 'Class',\n 'Type',\n 'Commission date',\n 'Homeport[2]',\n 'Note']\n\n\nBefore we construct the KG, we need to do a bit of data cleaning so we have a properly constructed Knowledge Graph. We will use ChatGPT “Advanced Data Analysis” as a surrogate for a Large Language Model based Agent.\n\nPrompt: Before we proceed, lets think critically about the dataframe structure. We will use the to_json method to construct a JSON document and then combine it with the context. However, there are some issues with the data, and in particular the format of dates in the dataframe. Think critically and look for issues in the pandas dataframe that will result in inconsistent or invalid KG construction.\n\nChatGPT: You’re correct; a critical review of the DataFrame structure is essential before proceeding to the knowledge graph construction. This will help ensure that the resulting KG is both consistent and valid. Here are some aspects to consider:\n\nIssues with the DataFrame:\n\nDate Format: The “Commission date” column is formatted as a string in “YYYY-MM-DD” format. While the to_json method would convert it into a string in JSON, it’s crucial that the context file specifies the date’s type as “xsd:date” to ensure that it’s interpreted correctly in the resulting KG.\nNotes Column: The “Note” column contains bracketed numbers, which could be references or footnotes. The meaning is unclear, and depending on the intended use, this could introduce ambiguity into the KG.\nHomeport Formatting: The “Homeport[2]” column contains location data as a string, but it’s not in a standardized geographical format. This could be an issue if geographical querying is required later.\nColumn Names: The column name “Homeport[2]” includes a special character, which may require special handling in both JSON and RDF. It’s generally good practice to use simple, descriptive column names.\nUniqueness: The “Hull number” is assumed to be a unique identifier for each ship, but the DataFrame doesn’t enforce this. If there are duplicates, this would result in invalid KG construction.\nImplicit Relationships: The data implicitly suggests relationships (e.g., a ship belongs to a class and type), but these are not explicitly modeled in the DataFrame. They will be modeled in the KG, but it’s good to be aware of these.\nRole Modeling: Since we decided to model “Commissioned Ship” as a role, the DataFrame structure doesn’t capture this. Specifically, roles like “Commissioned Ship” often have start and end dates, but our DataFrame only has the start date (“Commission date”).\nMissing Data: The DataFrame doesn’t indicate missing or null values explicitly, which could be an issue when constructing the KG.\nData Types: While pandas will infer the data types, it’s good to explicitly set them, especially for columns that should be interpreted as categorical data (e.g., “Class”, “Type”).\n\nAddressing these issues before KG construction will help ensure that the resulting KG is both valid and useful for downstream applications. Would you like to address these issues step-by-step?\n\n\n\nCode\nimport json\nimport uuid\n\nfrom pprint import pprint\n\n# Normalize column names in DataFrame\nnormalized_columns = {col: col.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\") for col in column_names}\ndf.rename(columns=normalized_columns, inplace=True)\n\n# Let's change the date to be consistent with xsd:date\n# df['Commission_date'] = pd.to_datetime(df['Commission_date']).dt.strftime('%Y-%m-%d')\n\n# Convert DataFrame to JSON-formatted string\ndf_json_str = df.to_json(orient=\"records\", indent=4)\n\n# Convert JSON-formatted string to Python object\ndf_json = json.loads(df_json_str)\n\n# Pretty-print the first record\npprint(df_json[0])\n\n\n{'Class': 'Nimitz',\n 'Commission_date': '11 November 1989',\n 'Homeport2': 'San Diego, CA',\n 'Hull_number': 'CVN-72',\n 'Note': '[3]',\n 'Ship_name': 'USS\\xa0Abraham Lincoln',\n 'Type': 'Aircraft carrier'}\n\nFigure 2: ?(caption)\n\n\nLooking at this JSON version of the first row of the dataframe, we can see some small data problems that we might want to address. First the “Commission_data” is not in an xsd:date format our knowledge graph would expect for a date literal. Second, the “note” field isn’t very useful without the note. For the “Null_number”, Homeport2, Class, Type and Ship Name, we will eventually want to align to wikidata properties. Remember, the Column names serve as properties to link “whatever” the row semantics are to the literal values of the table.\n\n\nCode\n\nfrom datetime import datetime\n\n\n# Data Cleaning Steps\n# 1. Replace special characters in 'Ship_name'\ndf['Ship_name'] = df['Ship_name'].str.replace('\\xa0', ' ')\n\n# 2. Standardize 'Commission_date' to ISO 8601 format\ndf['Commission_date'] = pd.to_datetime(df['Commission_date']).dt.strftime('%Y-%m-%d')\n\n# 3. Remove 'Note' field\ndf = df.drop(columns=['Note'])\n\n# 4. Standardize 'Homeport' field name\ndf.rename(columns={'Homeport2': 'Homeport'}, inplace=True)\n\n# Convert cleaned DataFrame to JSON\njson_data_cleaned = df.to_json(orient='records')\n\n# Convert JSON-formatted string to Python object\ndf_json = json.loads(json_data_cleaned)\n\n# Pretty-print the first record\npprint(df_json[0])\n\n\n{'Class': 'Nimitz',\n 'Commission_date': '1989-11-11',\n 'Homeport': 'San Diego, CA',\n 'Hull_number': 'CVN-72',\n 'Ship_name': 'USS Abraham Lincoln',\n 'Type': 'Aircraft carrier'}\n\n\n\n\n\n\n\n\nConventions for naming in RDF\n\n\n\nThe prevalent practice in RDF ontology naming is to employ Camel case. Specifically, lower camel case is preferred for relationships, which include properties and edges in the knowledge graph. In contrast, upper camel case is advised for Classes and types, representing nodes in the knowledge graph. It’s worth noting that while camel case serves as an implicit identifier, more explicit identifiers such as rdf:Property are also commonly used, especially in the context of large language models.\nFor example:\n{\n  \"@context\": {\n    \"@vocab\": \"http://schema.org/\",\n    \"name\": {\n      \"@id\": \"rdf:Property\",\n      \"@type\": \"xsd:string\"\n    }\n  },\n  \"@type\": \"Person\",\n  \"name\": \"John Doe\"\n}\n\n\nTo generate a RDF knowledge graph from the JSON, we also need a (context?) that defines our JSON keys.\n\n\n\nCode\nfrom rdflib import Graph, URIRef\n\n# Step 1: Add UUID and Type to JSON\njson_data_cleaned_dict = json.loads(json_data_cleaned)\nfor obj in json_data_cleaned_dict:\n    obj[\"UUID\"] = \"urn:uuid:\" + str(uuid.uuid4())\n    obj[\"@type\"] = \"Ship\"\n\n# Step 2: Create JSON-LD Context with SKOS definitions and XSD literals\ncontext_with_skos_and_xsd = {\n    \"@context\": {\n        \"UUID\": \"@id\",\n        \"Class\": {\n            \"@id\": \"http://example.org/Class\",\n            \"@type\": \"xsd:string\",\n            \"skos:definition\": \"The class of the ship.\"\n        },\n        \"Commission_date\": {\n            \"@id\": \"http://example.org/Commission_date\",\n            \"@type\": \"xsd:date\",\n            \"skos:definition\": \"The date the ship was commissioned.\"\n        },\n        \"Homeport\": {\n            \"@id\": \"http://example.org/Homeport\",\n            \"@type\": \"xsd:string\",\n            \"skos:definition\": \"The homeport of the ship.\"\n        },\n        \"Hull_number\": {\n            \"@id\": \"http://example.org/Hull_number\",\n            \"@type\": \"xsd:string\",\n            \"skos:definition\": \"The hull number of the ship.\"\n        },\n        \"Ship_name\": {\n            \"@id\": \"http://example.org/Ship_name\",\n            \"@type\": \"xsd:string\",\n            \"skos:definition\": \"The name of the ship.\"\n        },\n        \"Type\": {\n            \"@id\": \"http://example.org/Type\",\n            \"@type\": \"xsd:string\",\n            \"skos:definition\": \"The type of the ship.\"\n        },\n        \"@type\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\n        \"Ship\": \"http://example.org/Ship\"\n    }\n}\n\n# Step 3: Combine to Form JSON-LD\njson_ld_data_cleaned = {\n    \"@context\": context_with_skos_and_xsd[\"@context\"],\n    \"@graph\": json_data_cleaned_dict\n}\n\n# Step 4: Load into RDFlib\ng_cleaned = Graph()\ng_cleaned.parse(data=json.dumps(json_ld_data_cleaned), format='json-ld')\n\n# Step 5: Serialize to Turtle\nturtle_data = g_cleaned.serialize(format='turtle')\n\nFigure 3: ?(caption)\n\n\n\n\n\n@prefix ns1: &lt;http://example.org/&gt; .\n&lt;urn:uuid:0080c026-d273-450a-a687-5482a792447a&gt; a ns1:Ship ;\n    ns1:Class \"Ticonderoga\"^^&lt;xsd:string&gt; ;\n    ns1:Commission_date \"1991-01-12\"^^&lt;xsd:date&gt; ;\n    ns1:Homeport \"Pearl Harbor, HI\"^^&lt;xsd:string&gt; ;\n    ns1:Hull_number \"CG-65\"^^&lt;xsd:string&gt; ;\n    ns1:Ship_name \"USS Chosin\"^^&lt;xsd:string&gt; ;\n    ns1:Type \"Cruiser\"^^&lt;xsd:string&gt; .\n&lt;urn:uuid:00d02856-4403-45c4-a056-1e9e7ec826ee&gt; a ns1:Ship ;\n    ns1:Class \"Arleigh Burke\"^^&lt;xsd:string&gt; ;\n    ns1:Commission_date \"1995-10-14\"^^&lt;xsd:date&gt; ;\n    ns1:Homeport \"San Diego, CA\"^^&lt;xsd:string&gt; ;\n    ns1:Hull_number \"DDG-62\"^^&lt;xsd:string&gt; ;\n    ns1:Ship_name \"USS Fitzgerald\"^^&lt;xsd:string&gt; ;\n    ns1:Type \"Destroyer\"^^&lt;xsd:string&gt; .\n\nFigure 4: ?(caption)\n\n\n\n\nCode\n# Function to pretty-print Turtle string with indentation and line returns\ndef pretty_print_turtle(turtle_str):\n    lines = turtle_str.strip().split('\\n')\n    pretty_lines = []\n    \n    for line in lines:\n        if ';' in line and '.' in line:\n            triples = line.split(';')\n            formatted_triples = ' ;\\n    '.join(triple.strip() for triple in triples)\n            pretty_lines.append(formatted_triples.rstrip(' .') + ' .' )\n        else:\n            pretty_lines.append(line)\n\n    return '\\n'.join(pretty_lines)"
  }
]