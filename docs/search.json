[
  {
    "objectID": "02_jsonld.html",
    "href": "02_jsonld.html",
    "title": "Working with JSON-LD",
    "section": "",
    "text": "We have been using the kglab tutorial to learn about using W3C web standards to construct knowledge graphs. One particularly useful standard that is briefly touched on in the tutorial is JSON-LD in the serialization exercise. This corresponds to building a knowledge graph with rdflib in the kglab/examples subdirectory. JSON-LD context\n\n\nThe beauty of JSON-LD lies in its ability to transform standard JSON data into a knowledge graph simply and efficiently. By adding a ‘(context?)’ to a JSON document, developers can define how the data should be interpreted semantically. This turns a flat data structure into an interconnected web of information, opening up powerful querying and linking possibilities.\n\n\n\nThe release of JSON-LD 1.1 introduced a range of new features aimed at enhancing its functionality and ease of use. Key updates include improved context management, support for graph containers, and the ability to nest node objects. These features make it easier to construct intricate knowledge graphs and offer more control over how data is contextualized."
  },
  {
    "objectID": "02_jsonld.html#some-notes-on-json-ld",
    "href": "02_jsonld.html#some-notes-on-json-ld",
    "title": "Working with JSON-LD",
    "section": "",
    "text": "We have been using the kglab tutorial to learn about using W3C web standards to construct knowledge graphs. One particularly useful standard that is briefly touched on in the tutorial is JSON-LD in the serialization exercise. This corresponds to building a knowledge graph with rdflib in the kglab/examples subdirectory. JSON-LD context\n\n\nThe beauty of JSON-LD lies in its ability to transform standard JSON data into a knowledge graph simply and efficiently. By adding a ‘(context?)’ to a JSON document, developers can define how the data should be interpreted semantically. This turns a flat data structure into an interconnected web of information, opening up powerful querying and linking possibilities.\n\n\n\nThe release of JSON-LD 1.1 introduced a range of new features aimed at enhancing its functionality and ease of use. Key updates include improved context management, support for graph containers, and the ability to nest node objects. These features make it easier to construct intricate knowledge graphs and offer more control over how data is contextualized."
  },
  {
    "objectID": "02_jsonld.html#json-ld-and-web-apis",
    "href": "02_jsonld.html#json-ld-and-web-apis",
    "title": "Working with JSON-LD",
    "section": "JSON-LD and Web APIs",
    "text": "JSON-LD and Web APIs\nIn the modern web ecosystem, APIs serve as the bridges between different services and applications. JSON-LD’s compatibility with web APIs makes it a top choice for developers needing to consume or provide structured, linked data. Its seamless integration with RESTful services ensures that you can work within a familiar environment while benefiting from enhanced data semantics.\nMoreover, JSON-LD’s ability to express linked data allows for more advanced operations such as data aggregation, filtering, and transformation directly via API calls. This creates opportunities for developing richer, more interactive applications that can adapt in real-time to changes in underlying data. For example, by utilizing JSON-LD in a RESTful API for a content management system, you could dynamically link related articles, authors, and tags, thereby providing a more enriched user experience.\nAdditionally, JSON-LD’s interoperability means it can be easily coupled with other web standards like OAuth for secure authentication or CORS for cross-origin resource sharing. This makes it not just a data format, but a comprehensive solution for building robust and scalable API ecosystems.\nFinally, JSON-LD also plays a significant role in the realm of Web APIs for semantic search engines and linked data platforms. These APIs can consume JSON-LD to understand the contextual relationships between different pieces of information, thereby enabling more intelligent and nuanced search queries."
  },
  {
    "objectID": "02_jsonld.html#example-exposing-json-ld-context-via-http-link-header",
    "href": "02_jsonld.html#example-exposing-json-ld-context-via-http-link-header",
    "title": "Working with JSON-LD",
    "section": "Example: Exposing JSON-LD Context via HTTP Link Header",
    "text": "Example: Exposing JSON-LD Context via HTTP Link Header\nIn many real-world applications, the JSON-LD context can be exposed via an HTTP link header, similar to how Schema.org does it. This enables clients to discover the context automatically and understand how to interpret the linked data.\nSuppose you have a RESTful API for a blog platform, and you want to expose a JSON-LD context for articles. The HTTP response could include a link header pointing to the JSON-LD context:\nHTTP/1.1 200 OK\nContent-Type: application/json\nLink: &lt;https://yourapi.com/docs/jsonldcontext.json&gt;; rel=\"http://www.w3.org/ns/json-ld#context\"; type=\"application/ld+json\"\nWith this setup, clients consuming the API can follow the link to fetch the context and understand the semantics of the data. Here is a simplified example of what the jsonldcontext.json might look like:\n{\n  \"@context\": {\n    \"title\": \"http://schema.org/headline\",\n    \"author\": \"http://schema.org/author\",\n    \"datePublished\": \"http://schema.org/datePublished\",\n    \"content\": \"http://schema.org/text\"\n  }\n}\nBy using the link header to expose the JSON-LD context, you’re making it easier for clients to consume and understand your API’s data. This aligns well with JSON-LD 1.1 conventions and allows for greater interoperability and semantic richness."
  },
  {
    "objectID": "02_jsonld.html#example-using-json-ld-to-construct-a-knowledge-graph-from-wikipedia-tables",
    "href": "02_jsonld.html#example-using-json-ld-to-construct-a-knowledge-graph-from-wikipedia-tables",
    "title": "Working with JSON-LD",
    "section": "Example: Using JSON-LD to construct a Knowledge Graph from Wikipedia tables",
    "text": "Example: Using JSON-LD to construct a Knowledge Graph from Wikipedia tables\nWikipedia can provide a starting point for structuring and enriching knowledge graphs. For our Navy project, we will want to provide LLM Agents with context for factual information that may have changed since “pre-training” using Retrieval Augmented Generation. For our purposes, we want to augment our KG with broader contextual information that may be useful to agents for question and answering. One set of context, is the List of current ships of the United States Navy that contain a number of tables that could be useful to add to our knowledge graph. Wikidata does contain most of the ships but the entities are not always complete. For example, the Wikidata entity page for USS Abraham Lincoln shows a rather rich set of relations including events that correspond to ship naming ceremony, ship commissioning. However, ships like USS Carter Hall are very basic and don’t contain a lot of information we can extract. Looking at the History for List of current ships of the United States Navy tells us that the page is fairly active and Wikipedia contributors are keeping it more up to date than the Wikidata entries.\nLets’ do a little Exploratory Data Analysis using large language models similar to Getting Started With LLMs. We will use Beautiful Soup to retreive the Wikipedia page and find the “html tables” in the document.\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl = \"https://en.wikipedia.org/wiki/List_of_current_ships_of_the_United_States_Navy\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\ntable = soup.find_all(\"table\")[0]\n\n\nNow that we have the tables, lets create a pandas dataframe from the table and sanity check that the table is the same as what is in the first table.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_html(str(table))[0]\ndf\n\n\n/tmp/ipykernel_4540/553751003.py:3: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table))[0]\n\n\n\n\n\n\n\n\n\n\nShip name\nHull number\nClass\nType\nCommission date\nHomeport[2]\nNote\n\n\n\n\n0\nUSS Abraham Lincoln\nCVN-72\nNimitz\nAircraft carrier\n11 November 1989\nSan Diego, CA\n[3]\n\n\n1\nUSS Alabama\nSSBN-731\nOhio\nBallistic missile submarine\n25 May 1985\nBangor, WA\n[4]\n\n\n2\nUSS Alaska\nSSBN-732\nOhio\nBallistic missile submarine\n25 January 1986\nKings Bay, GA\n[5]\n\n\n3\nUSS Albany\nSSN-753\nLos Angeles\nAttack submarine\n7 April 1990\nNorfolk, VA\n[6]\n\n\n4\nUSS Alexandria\nSSN-757\nLos Angeles\nAttack submarine\n29 June 1991\nSan Diego, CA\n[7] Scheduled to be decommissioned 2026[8]\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n234\nUSS William P. Lawrence\nDDG-110\nArleigh Burke\nDestroyer\n19 May 2011\nSan Diego, CA\n[242]\n\n\n235\nUSS Winston S. Churchill\nDDG-81\nArleigh Burke\nDestroyer\n10 March 2001\nNorfolk, VA\n[243]\n\n\n236\nUSS Wichita\nLCS-13\nFreedom\nLittoral combat ship\n12 January 2019\nMayport, FL\n[244] Proposed to be decommissioned 2023[17]\n\n\n237\nUSS Wyoming\nSSBN-742\nOhio\nBallistic missile submarine\n13 July 1996\nKings Bay, GA\n[245]\n\n\n238\nUSS Zumwalt\nDDG-1000\nZumwalt\nDestroyer\n15 October 2016\nSan Diego, CA\n[246]\n\n\n\n\n239 rows × 7 columns\n\nFigure 1: Dataframe from Wikipedia Commissioned ship table.\n\n\n\nSo, if we want to use the column names as the basis for eventually constructing a URI, we unfortunately need to make it web safe and remove spaces and other issues. The other question is “what does a row” represent? If we want to build a knowledge graph of all of the ships of the navy, we might want to consider a basic ontology to start with. The first table is “commissioned” ships and the idea of commisioned is a role relationship since it has a start time and a end time.\n\n\nCode\ncolumn_names = df.columns.tolist()\ncolumn_names\n\n\n['Ship name',\n 'Hull number',\n 'Class',\n 'Type',\n 'Commission date',\n 'Homeport[2]',\n 'Note']\n\n\n\n\n\nCode\nimport json\nimport uuid\n\nfrom pprint import pprint\n\n# Normalize column names in DataFrame\nnormalized_columns = {col: col.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\") for col in column_names}\ndf.rename(columns=normalized_columns, inplace=True)\n\n# Let's change the date to be consistent with xsd:date\n# df['Commission_date'] = pd.to_datetime(df['Commission_date']).dt.strftime('%Y-%m-%d')\n\n# Convert DataFrame to JSON-formatted string\ndf_json_str = df.to_json(orient=\"records\", indent=4)\n\n# Convert JSON-formatted string to Python object\ndf_json = json.loads(df_json_str)\n\n# Pretty-print the first record\npprint(df_json[0])\n\n\n{'Class': 'Nimitz',\n 'Commission_date': '11 November 1989',\n 'Homeport2': 'San Diego, CA',\n 'Hull_number': 'CVN-72',\n 'Note': '[3]',\n 'Ship_name': 'USS\\xa0Abraham Lincoln',\n 'Type': 'Aircraft carrier'}\n\nFigure 2: ?(caption)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Knowledge Graphs",
    "section": "Introduction",
    "text": "Introduction\nWelcome to FastKG-Course, an interactive course designed to make you proficient in Knowledge Graphs, Semantic Technologies, Ontology, and more. This project is affiliated with the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Knowledge Graphs",
    "section": "Features",
    "text": "Features\n\nInteractive Jupyter Notebooks\nHands-on Tutorials\nUse of fast.ai and kglab libraries"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Knowledge Graphs",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\n(Optional) Fork this repository\nOpen the project in GitHub Codespaces\nThe devcontainer will automatically set up your environment\nStart the Jupyter server and open the introductory notebook"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Knowledge Graphs",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this course, it is highly recommended to complete the following Fast.ai courses:\n\nPractical Deep Learning for Coders\nDeep Learning for Coders with fastai & Pytorch\nJupyter Notebook 101\n\nAdditional requirements:\n\nBasic knowledge of Python\nFamiliarity with machine learning concepts"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Knowledge Graphs",
    "section": "Usage",
    "text": "Usage\nNavigate through the Jupyter notebooks in sequence for a curated educational path, or feel free to explore topics that interest you.\nThe dev container is fully configured with software and KG and machine learning libraries needed for this course."
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Knowledge Graphs",
    "section": "Contributors",
    "text": "Contributors\n\nCharles F Vardeman II"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Knowledge Graphs",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis project is part of the Laboratory for Assured AI Applications Development in the Center for Research Computing at the University of Notre Dame. The kglab project for providing some integrated tools for working with Knowledge Graphs as well as an excellent tutorial. Content generation and assistance have been facilitated using OpenAI’s GPT-4 language model. ChatGPT August 3, 2023 version"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Knowledge Graphs",
    "section": "License",
    "text": "License\nThis project is dual-licensed under:\n\nMIT License for the software components. See LICENSE-MIT for more details.\nCreative Commons Attribution 4.0 International (CC BY 4.0) for the educational content. See LICENSE-CC-BY for more details.\nThe kglab submodule from derwen.ai is licensed under the MIT License.\n\nYou are free to use the project under either of the licenses, depending on your needs."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "Quickstart to Knowledge Graphs with Semantic Technologies: A fast.ai Approach",
    "section": "",
    "text": "Welcome to this hands-on tutorial, engineered to accelerate your grasp of Knowledge Engineering, Knowledge Graphs, Linked Data, and the role of Generative AI in factual enrichment. We adopt the pedagogical philosophy of fast.ai, as articulated in their blog post Providing a Good Education in Deep Learning, to make the realm of Knowledge Engineering as approachable as neural networks have become.\n\nWhy the fast.ai Approach?\nThe fast.ai philosophy emphasizes learning by doing, with a top-down approach that introduces complex topics through practical examples and projects. This hands-on, application-first methodology allows for quick, tangible results that not only bolster understanding but also demonstrate the utility of the subject matter. By incorporating these principles, we aim to demystify the seemingly arcane field of Knowledge Engineering and to deliver both the ‘why’ and the ‘how’ in a manner that encourages proactive learning.\n\n\nPrerequisites\nTo harmonize with the fast.ai approach, this tutorial leverages some of their key introductory materials. Familiarity with the following topics is strongly recommended:\n\nGetting Started with Practical Deep Learning\nJupyter Notebook 101\nDeep Learning Deployment\nNeural Network Foundations\nNatural Language Processing\n\n\n\n\n\n\n\nGenerative AI as Learning Companions\n\n\n\nGenerative AI agents like ChatGPT, Bing Chat, and Bard offer proficiency in Semantic Web technologies and Python tools like rdflib. They serve as invaluable ‘copilots’ for code generation, debugging, and problem-solving and will be incorporated into later notebooks for ontology construction. Generative AI can also assist with revising text in markdown cells serving as an editor for content.\n\n\n\n\nThe Legacy of The Semantic Web\nThe vision for a semantic web was first articulated by Tim Berners-Lee and his co-authors in their landmark paper “The Semantic Web” published in Scientific American (Berners-Lee, Hendler, and Lassila 2001). This pioneering work envisaged a web ecosystem where Software Agents, Shared Ontologies, and Linked Data would serve as the backbone for assistive agents and intelligent systems. Our tutorial aligns with this vision and the standards that have evolved from it, such as RDF and OWL.\n\nBerners-Lee, Tim, James Hendler, and Ora Lassila. 2001. “The Semantic Web.” Scientific American 284 (5): 34–43. https://www.jstor.org/stable/26059207.\n\n\nFurther Reading\nFor an in-depth technical understanding, we recommend “A data engineer’s guide to semantic modeling” by Ilaria Maresi (@ Maresi 2020) as key references.\n\nMaresi, Ilaria. 2020. A Data Engineer’s Guide to Semantic Modelling. Zenodo. https://doi.org/10.5281/zenodo.3898519.\n\n\nKnowledge Graphs: What You Need to Know\nFor a more technical understanding, the book “Knowledge Graphs” (Hogan et al. 2021) offers a definitive guide. Key chapters to review include:\n\nHogan, Aidan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard de Melo, Claudio Gutiérrez, Sabrina Kirrane, et al. 2021. Knowledge Graphs. Synthesis Lectures on Data, Semantics, and Knowledge 22. Springer. https://doi.org/10.2200/S01125ED1V01Y202109DSK022.\n\nIntroduction, Data Graphs, Creation and Enrichment, and Publication\n\nAdditional recommended chapters include Schema, Identity, Context and Deductive Knowledge.\n\n\n\n\n\n\nNote\n\n\n\nWe use the “Knowledge Graph” books definition of a knowledge graph from the introduction:\n\n“… as a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent relations between these entities.”\n\nThis definition is broad enough to include embedding structured Knowledge Graph databases like KùzuDB and Retrieval Augmented Generation (RAG) in frameworks like LlamaIndex that can automate LLM based query retrieval using knowledge graphs with Large Language Models.\n\n\n\n\nTooling Up\nDerwen.ai’s Python package kglab will serve as our primary toolset for building knowledge graphs. The kglab tutorial is a complementary resource worth exploring. We have included the tutorial as a [Git Submodule](https://github.blog/2016-02-01-working-with-submodules/ in the kglab directory of this repository.\nWe invite you to immerse yourself in the provided examples for hands-on practice and a more comprehensive understanding of knowledge graphs.\n\nData and Ontology sources - kglab (derwen.ai) in kglab/examples/ex0_0.pynb\nBuild an RDF graph using RDFlib - kglab (derwen.ai) in kglab/examples/ex1_0.pynb\nLeverage the kglab abstraction layer - kglab (derwen.ai) in in kglab/examples/ex1_1.pynb\nBuild a medium size KG from a CSV dataset - kglab (derwen.ai) in in kglab/examples/ex2_0.pynb\nRun SPARQL queries - kglab (derwen.ai) in kglab/examples/ex4_0.pynb\nInteractive graph visualization with PyVis - kglab (derwen.ai) in in kglab/examples/ex3_0.pynb\nMeasurement and inference - kglab (derwen.ai) in kglab/examples/ex7_0.pynb\n\nThe Devcontainer in the code space should automatically install from the .devcontainer/requirements.txt fastai, kglab and pandas needed for other parts of this tutorial. To check these are installed correctly, the following code block\n\n# Import libraries\nimport fastai\nimport kglab\nimport pandas\n\n# Print versions\nprint(f'fastai version: {fastai.__version__}')\nprint(f'kglab version: {kglab.__version__}')\nprint(f'pandas version: {pandas.__version__}')\n\nfastai version: 2.7.12\nkglab version: 0.6.6\npandas version: 2.1.0"
  }
]